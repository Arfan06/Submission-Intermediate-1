# -*- coding: utf-8 -*-
"""Customer_churn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mWXUgZ5VrFWoqUWI1UIbERd5MjXVwfwv

# Proyek Customer Churn
- **Nama:** [Ari Fansuri]
- **Email:** [arfansurti26@gmail.com]

## Import Semua Packages/Library yang Digunakan

Mengimpr dan instal semua library yang dibutuhkan.
"""

import os, shutil
import zipfile
from google.colab import files
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import joblib
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, classification_report
from sklearn.linear_model import LogisticRegression
!pip install ctgan
from ctgan import CTGAN

"""## Data Preparation

Mengimpor dataset dari kaggle dan melakukan ekstrak data dari format berekstensi zip.
"""

files.upload()

# Import dataset dari kaggle
!kaggle datasets download -d radheshyamkollipara/bank-customer-churn -p /content/

# Ekstrak file yang sudah diunduh
!unzip bank-customer-churn.zip

"""## Data Understanding

## Data Loading

Memuat data yang telah diekstrak dan melakukan assign ke variabel baru bernama df.
"""

# Memuat dataset yang sudah dipersiapkan
df = pd.read_csv('/content/Customer-Churn-Records.csv')

# Memastikan header dan 5 baris teratas dataset
df.head()

# Memastikan header dan 5 baris dataset teratas-tertabawah
df

"""Berdasarkan data yang ditampilkan, terdapat 3 kolom yang dinyatakan sebagai jenis data kategorikal dan 3 tipe data boolean.

## EDA

Bagian Exploratory Data Analysis (EDA) ini bertujuan untuk mengeksplorasi dataset secara mendalam guna memahami berbagai karakteristik, pola, serta hubungan antar variabel yang mungkin ada. EDA membantu mengidentifikasi tren, anomali, serta pola distribusi, sekaligus menemukan korelasi yang relevan untuk model atau keputusan yang akan diambil. Dengan menggunakan visualisasi data dan statistik deskriptif, dapat menggali lebih dalam untuk mendapatkan wawasan insight berharga yang dapat mendukung langkah pemodelan dan pengambilan keputusan berikutnya.

### Deskripsi Variabel
"""

# Memastikan informasi tipe data pada masing-masing kolom
df.info()

"""Dari informasi fitur data diatas, terdapat kesalahan dalam penulisan kolom data, beberapa kolom memiliki karakter penghubung menggunakan spasi, yaitu kolom satisfaction score, card type dan point earned. Untuk memudahkan pemrosesan data, kolom-kolom tersebut akan diubah menggunkan karakter underscore sebagai penghubungnya."""

# Melakukan analisis statistik sederhana pada kolom data numerikal
df.describe()

"""Berdasarkan informasi statistik sederhana diatas ditemukan bahwa;
* **Usia** tertinggi nasabah adalah 92 tahun dan yang termuda adalah 18 tahun.
* **Kredit skor** tertinggi adalah 850 poin, terendah 350 poin dan rata-rata kredit skor yang diperoleh nasabah adalah 650,5 poin.
* **Balance** tertinggi yang dimiliki oleh nasabah adalah 250.898,09 USD, terendah 0 USD dan rata-rata sebesar 76.485,88 USD.
* Estimasi **salary** nasabah paling tinggi sebesar 199.992,48 USD, terendah 11,58 USD dan rata-rata sebesar 100.090,23 USD.
* Skor **kepuasan** yang diberikan oleh nasabah rata-rata 3,01 poin.
"""

# Mengubah nama kolom dengan spasi menjadi underscore
df.columns = [col.replace(' ', '_') for col in df.columns]

# Menampilkan nama kolom setelah perubahan
print("Nama kolom setelah perubahan:")
print(df.columns)

"""Kolom-kolom yang mengalami perubahan diantaranya adalah; **Satisfaction_Score**, **Card_Type** dan **Point_Earned**.

### Missing Value dan Outliers
"""

# Memastikan sel berisi angka nol
nol_value = (df == 0).sum()
print(nol_value)

"""Berdasarkan hasil pengecekan pada dataset, beberapa kolom yang berisi nilai nol "0" merupakan hasil transformasi data yang sebelumnya adalaha tipe data boolean. Sehingga nilai nol "0" pada dataframe expected sebagai representasi dari nilai False/No."""

# Memastikan kolom berisi data Null
df.isnull().sum()

"""Berdasarkan hasil pengecekan pada dataframe, tidak ditemukan adanya sel bernilai "null"/Nan/Kosong."""

# Menentukan kolom numerik kecuali 'CustomerId' dan 'RowNumber'
numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns
numerical_columns = [col for col in numerical_columns if col not in ['CustomerId', 'RowNumber', 'IsActiveMember', 'Exited', 'Complain', 'HasCrCard', 'NumOfProducts']]

# Menyiapkan ukuran plot
plt.figure(figsize=(15, len(numerical_columns) * 2))

# Membuat boxplot untuk setiap kolom numerik yang dipilih
for i, column in enumerate(numerical_columns, 1):
    plt.subplot(len(numerical_columns), 1, i)
    sns.boxplot(x=df[column])
    plt.title(f'Boxplot of {column}')

plt.tight_layout()
plt.show()

"""Berdasarkan hasil identifikasi outlier menggunakan boxplot, tidak ditemukan adanya outlier dan memang dianggap sebagai anomali yang sah dalam sebaran data. Berikut keterangan pada masing-masing fitur:

* Credit score: Outlier pada data credit score dianggap normal karena beberapa customer memiliki kredit score yang rendah.
* Age: Outlier pada kolom Age dianggap normal karena usia customer berada pada rentang yang dianggap normal sebagai usia manusia.
* Balance: Tidak ditemukan adanya outlier, namun kemungkinan besar data memiliki nilai rendah yang cukup dominan, sehingga distribusi data menjadi tidak normal dan mendorong box plot mendekati nilai minimum.

### Univariate Analysis
"""

# Fungsi untuk menampilkan grafik batang dengan persentase sebagai teks
def plot_category_counts(column_name):
    # Menghitung jumlah kategori dan persentase
    counts = df[column_name].value_counts()
    percentages = (counts / counts.sum()) * 100

    # Seting gaya grafik
    sns.set(style="whitegrid")
    plt.figure(figsize=(10, 6))

    # Membuat grafik batang
    ax = sns.barplot(x=counts.index, y=counts.values, palette="viridis")

    # Menambahkan judul dan label
    plt.title(f'Jumlah dan Persentase Kategori pada Kolom {column_name}', fontsize=16, fontweight='bold')
    plt.xlabel(column_name, fontsize=14)
    plt.ylabel('Jumlah', fontsize=14)

    # Menambahkan persentase sebagai teks di atas setiap batang
    for i, (count, percent) in enumerate(zip(counts, percentages)):
        ax.text(i, count + 0.5, f'{percent:.2f}%', ha='center', color='black', fontsize=12)

    # Menampilkan grafik
    plt.show()

# Memanggil fungsi dengan kolom yang diinginkan
plot_category_counts('Geography')

"""Jumlah nasabah berkewarganegaraan Prancis mendominasi keseluruhan jumlah customer, yaitu sebanyak 50,14%. Sedangkan untuk Jerman dan Spanyol memiliki jumlah persentase yang hampir sama, yaitu sekitar 25%."""

# Fungsi untuk menampilkan grafik batang dengan persentase sebagai teks
def plot_category_counts(column_name):
    # Menghitung jumlah kategori dan persentase
    counts = df[column_name].value_counts()
    percentages = (counts / counts.sum()) * 100

    # Seting gaya grafik
    sns.set(style="whitegrid")
    plt.figure(figsize=(10, 6))

    # Membuat grafik batang dengan palet warna yang lebih menarik
    ax = sns.barplot(x=counts.index, y=counts.values, palette="coolwarm", alpha=0.8)

    # Menambahkan judul dan label
    plt.title(f'Jumlah dan Persentase Kategori pada Kolom {column_name}', fontsize=16, fontweight='bold')
    plt.xlabel(column_name, fontsize=14)
    plt.ylabel('Jumlah', fontsize=14)

    # Menambahkan persentase sebagai teks di atas setiap batang
    for i, (count, percent) in enumerate(zip(counts, percentages)):
        ax.text(i, count + 0.5, f'{percent:.2f}%', ha='center', color='black', fontsize=12)

    # Menampilkan grafik
    plt.show()

# Memanggil fungsi dengan kolom yang diinginkan
plot_category_counts('Card_Type')

"""Pada masing-masing tipe kartu memiliki besaran data yang seimbang di range sekitar 25% atau sekitar 2500 pengguna per-tipe kartu."""

# Fungsi untuk menampilkan grafik batang dengan persentase sebagai teks
def plot_category_counts(column_name):
    # Menghitung jumlah kategori dan persentase
    counts = df[column_name].value_counts()
    percentages = (counts / counts.sum()) * 100

    # Seting gaya grafik
    sns.set(style="whitegrid")
    plt.figure(figsize=(10, 6))

    # Membuat grafik batang dengan palet warna yang lebih menarik
    ax = sns.barplot(x=counts.index, y=counts.values, palette="viridis", alpha=0.9)

    # Menambahkan judul dan label
    plt.title(f'Jumlah dan Persentase Kategori pada Kolom {column_name}', fontsize=16, fontweight='bold')
    plt.xlabel(column_name, fontsize=14)
    plt.ylabel('Jumlah', fontsize=14)

    # Menambahkan persentase sebagai teks di atas setiap batang
    for i, (count, percent) in enumerate(zip(counts, percentages)):
        ax.text(i, count + 0.02 * max(counts), f'{percent:.2f}%', ha='center', color='black', fontsize=12)

    # Menampilkan grafik
    plt.show()

# Memanggil fungsi dengan kolom yang diinginkan
plot_category_counts('Gender')

"""Pengguna dengan jenis kelamin laki-laki mendominasi sebesar 54,57% dan pengguna dengan jenis kelamin perempuan sebesar 45,43%.

### Multivariate Analysis
"""

sns.pairplot(df[['CreditScore', 'Geography', 'Gender', 'Age', 'Balance', 'EstimatedSalary', 'Exited', 'Satisfaction_Score', 'Point_Earned', 'NumOfProducts', 'Complain']],
             hue='Exited',
             diag_kind='kde',
             kind='scatter',
             palette='husl')
plt.show()

"""Dari hasil analisis menggunakan pairplot, fitur seperti NumOfProducts, Age, dan Balance masih menunjukkan perbedaan yang berarti antara pelanggan yang churn dan yang tidak churn.  

Fitur Complain menambah perspektif, karena pelanggan dengan keluhan lebih mungkin untuk churn, terutama jika keluhan tersebut berhubungan dengan faktor kepuasan.
"""

# Menghitung matriks korelasi hanya untuk kolom numerik
correlation_matrix = df.select_dtypes(include=np.number).corr()

# Membuat heatmap korelasi
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Heatmap of Correlation Matrix')
plt.show()

"""Heatmap di atas menunjukkan matriks korelasi antara berbagai fitur dalam dataset. Korelasi diukur dalam rentang -1 hingga 1, di mana:

- **Nilai 1 (merah terang)** menunjukkan korelasi positif yang sempurna antara dua fitur, artinya peningkatan satu variabel cenderung diikuti oleh peningkatan variabel lainnya.
- **Nilai -1 (biru terang)** menunjukkan korelasi negatif yang sempurna, di mana peningkatan satu variabel cenderung diikuti oleh penurunan variabel lainnya.
- **Nilai mendekati 0 (biru gelap)** menunjukkan tidak adanya atau sedikit korelasi antara dua fitur.  

  

*Temuan Utama:*
1. **Korelasi tertinggi**: Complain memiliki sempurna dengan variabel `Exited`, yaitu sebesar 1 yang menunjukan adanya variabel positif yang menunjukan kecenderungan bahwa customer yang melakukan complain akan sangat mungkin melakukan Churn.
   
2. **Korelasi lainnya**:
   - Korelasi lainnya dalam dataset ini adalah antara **Age** dan **Exited** dengan nilai 0.29, menunjukkan bahwa usia memiliki hubungan positif yang lemah dengan variabel target `Exited`.
   
3. **Korelasi Negatif**:
   - Ada korelasi negatif antara **IsActiveMember** dan **Exited** (-0.16), menunjukkan bahwa pelanggan yang aktif cenderung memiliki peluang lebih rendah untuk keluar dari layanan. Namun, nilai korelasinya juga cukup rendah.

4. **Interaksi Fitur Lainnya**: Sebagian besar fitur memiliki korelasi sangat rendah satu sama lain, seperti **CreditScore** terhadap fitur lain yang hampir semuanya berada di sekitar 0. Ini menunjukkan bahwa sebagian besar fitur cukup independen.
"""

# Daftar variabel kategorikal dan numerik
categorical_vars = ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember', 'Complain']
numerical_vars = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary', 'Satisfaction_Score', 'Point_Earned']

# Mengatur ukuran figure
plt.figure(figsize=(15, 8))

# Plot perbandingan untuk variabel kategorikal
for i, var in enumerate(categorical_vars, 1):
    plt.subplot(2, 3, i)
    sns.countplot(data=df, x=var, hue='Exited')
    plt.title(f'Exited vs {var}')
    plt.legend(title='Exited', loc='upper right')

plt.tight_layout()
plt.show()

# Plot perbandingan untuk variabel numerik
plt.figure(figsize=(15, 12))
for i, var in enumerate(numerical_vars, 1):
    plt.subplot(3, 3, i)
    sns.boxplot(data=df, x='Exited', y=var)
    plt.title(f'Exited vs {var}')

plt.tight_layout()
plt.show()

"""Dari hasil analisis `perbandingan` variabel kategorikal, beberapa variabel yang tampaknya memiliki hubungan kuat dengan churn adalah Geography (terutama Jerman), Gender (perempuan), IsActiveMember (keanggotaan aktif), dan Complain (pelanggan yang pernah komplain). Variabel HasCrCard tampaknya kurang memberikan pengaruh signifikan terhadap churn.  

Dari hasil analisis variabel numerik vs exited menggunakan `boxplot`, Age tampak sebagai variabel yang paling relevan dalam membedakan antara pelanggan yang churn dan yang tidak. Variabel lain seperti NumOfProducts juga menunjukkan sedikit perbedaan, namun tidak sejelas variabel Age. Variabel lainnya, seperti CreditScore, Tenure, Balance, EstimatedSalary, Satisfaction_Score, dan Point_Earned, menunjukkan distribusi yang hampir serupa untuk kedua kelompok dan mungkin memiliki pengaruh yang lebih kecil terhadap churn.

### Feature Selection

## Data Preprocessing

### Drop Feature

Pada tahap ini, beberapa langkah pra-pemrosesan diterapkan pada dataset agar model lebih mudah dalam memahami data:

1. **Menghapus Kolom yang Tidak Relevan**: Kolom `RowNumber`, `Surname`, dan `CustomerId` dihapus dari dataset karena tidak berkontribusi langsung terhadap prediksi.

2. **Encoding Variabel Kategorikal**: Kolom kategorikal seperti `Geography`, `Gender`, dan `Card_Type` dikonversi menjadi bentuk numerik menggunakan teknik One-Hot Encoding. Hasil encoding ini disimpan dalam variabel `hot`.

3. **Menggabungkan Hasil Encoding**: Dataset utama `df` dikombinasikan dengan data hasil encoding (`hot`) sehingga informasi dalam variabel kategorikal sekarang tersimpan sebagai kolom numerik.

4. **Menghapus Kolom Kategorikal Asli**: Setelah encoding, kolom asli `Geography`, `Gender`, dan `Card_Type` dihapus dari dataset untuk menghindari redundansi.

5. **Mempersiapkan Fitur dan Label**: Semua kolom kecuali `Exited` diambil sebagai fitur (`X`), sementara kolom `Exited` dijadikan label (`y`). Fitur `X` diubah ke dalam bentuk array untuk memudahkan proses pelatihan model.
"""

# Menghapus kolom yang tidak relevan
df = df.drop(['RowNumber', 'Surname', 'CustomerId'], axis=1)

# Encoding kolom data kategorikal
encoder = pd.get_dummies(df[['Geography', 'Gender', 'Card_Type']])
hot = encoder

# Menghapus kolom kategorikal
df = pd.concat([df, hot], axis = 1)
df = df.drop(['Geography', 'Gender', 'Card_Type'], axis = 1)

# Menginisialisasi Kolom exited sebagai target
X = df.drop('Exited', axis = 1)
X = X.values
y = df['Exited']

"""### Split Dataset

Pada project ini, dataset dibagi menjadi dua bagian, yaitu data pelatihan dan data pengujian, dengan perbandingan 80:20. Artinya, 80% dari data digunakan untuk melatih model, sementara 20% sisanya digunakan untuk menguji performa model. Pembagian ini bertujuan untuk memastikan bahwa model dapat mempelajari pola dari data pelatihan, dan kemudian diuji pada data pengujian yang belum pernah dilihat model sebelumnya.
"""

# Membagi data menjadi data latih dan data uji
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Melakukan standarisasi fitur
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # Fit dan transform data latih
X_test_scaled = scaler.transform(X_test)  # Hanya transform data uji

# Menampilkan hasil standarisasi
print("X_train_scaled:\n", X_train_scaled[:5])  # Menampilkan 5 data pertama setelah scaling
print("X_test_scaled:\n", X_test_scaled[:5])  # Menampilkan 5 data pertama setelah scaling

"""## Model Development

### Algoritma random forest
"""

# Menentukan parameter model
param_grid = {
    'n_estimators': [100, 200, 500],
    'min_samples_split': [2, 5, 10],
    'max_depth': [10, 15, 20],
    'criterion': ['gini', 'entropy']
}
grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=0), param_grid=param_grid, cv=5, n_jobs=-1)
grid_search.fit(X_train, y_train)
best_params = grid_search.best_params_

model = RandomForestClassifier()
gridRandomForest = RandomizedSearchCV(model, param_grid, cv = 5, n_jobs = -1)
gridRandomForest.fit(X_train, y_train)

print('Algorithm: ', gridRandomForest.best_estimator_.criterion)
print('Score: ', gridRandomForest.best_score_)
print('Mín Split: ', gridRandomForest.best_estimator_.min_samples_split)
print('Max Nvl: ', gridRandomForest.best_estimator_.max_depth)

"""- **Algoritma**: Gini
  - Model ini menggunakan algoritma **Decision Tree** dengan **impurity criterion** berdasarkan **Gini index** untuk memisahkan data.

- **Score**: 0.9985
  - Skor model ini mencapai **99.85%**, menunjukkan performa yang sangat baik dalam mengklasifikasikan data dengan akurasi tinggi.

- **Min Split**: 2
  - Model memiliki parameter **minimum split** sebesar **2**, yang artinya sebuah node akan terbagi jika ada setidaknya 2 sample yang dapat dipisahkan.

- **Max Depth**: 20
  - **Kedalaman maksimum** (maximum depth) dari pohon keputusan ini dibatasi hingga **20 level**, untuk mengontrol ukuran pohon dan menghindari overfitting.

"""

# Membangun model
rf_model = RandomForestClassifier(**best_params, random_state=0)
rf_model.fit(X_train, y_train)
previsoes = rf_model.predict(X_test)

# Kalkulasi confussion matrix
cm = confusion_matrix(y_test, previsoes)

# Membuat display confussion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm)

# Memanggil gambar confussion matrix
disp.plot()
plt.show()

"""Berdasarkan model yang dibuat menggunakan algoritma random forest, diperoleh  confussion matrix dengan hasil performa yang sangat baik. Model mampu mengklasifikasikan kesalahan hanya pada 1 data, baik pada data yang diprediksi maupun pada data yang sebenarnya."""

# Prediksi data uji
y_pred = rf_model.predict(X_test)

# Evaluasi model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy on test data:", accuracy)

# Menampilkan laporan klasifikasi dan confussion matrix
print("\nClassification Report:\n", classification_report(y_test, y_pred))

"""Model yang diuji menunjukkan hasil yang sangat baik pada data uji dengan akurasi 99.9%. Ini berarti bahwa model berhasil mengklasifikasikan hampir semua sampel dengan benar. Rata-rata untuk semua metrik (macro avg dan weighted avg) menunjukkan nilai sempurna (1.00), mengindikasikan kinerja yang konsisten di seluruh kelas.

Secara keseluruhan, model ini memiliki performa yang sangat tinggi dalam mendeteksi kedua kelas pada data uji, dengan tingkat kesalahan yang mendekati nol.
"""

# Menampilkan fitur penting
feature_importances = rf_model.feature_importances_
feature_names = df.drop('Exited', axis=1).columns

# Plot
plt.figure(figsize=(12, 6))
indices = np.argsort(feature_importances)[::-1]
plt.bar(range(X_train.shape[1]), feature_importances[indices], align='center')
plt.xticks(range(X_train.shape[1]), feature_names[indices], rotation=90)
plt.title('Feature Importances')
plt.show()

"""Hasil yang diperoleh dari model yang dibuat, ditemukan bahwa variabel komplain memberikan pengaruh yang paling tinggi terhadap variabel exited nasabah dan diikuti oleh variabel usia, jumlah produk dan balance setelahnya.

### Algoritma logistic regression
"""

# Inisialisasi model Logistic Regression
log_reg = LogisticRegression(random_state=0)

# Latih model
log_reg.fit(X_train, y_train)

# Prediksi pada data uji
y_pred = log_reg.predict(X_test)

# Menghitung akurasi
accuracy = accuracy_score(y_test, y_pred)
print(f"Akurasi pada data uji: {accuracy:.2f}")

# Menampilkan confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix on Test Data')
plt.show()

# Menampilkan classification report
report = classification_report(y_test, y_pred)
print("Classification Report:")
print(report)

"""Model **Random Forest** yang diuji sebelumnya menunjukkan hasil yang jauh lebih baik dengan akurasi sebesar 99.85% dan kinerja yang sangat baik pada kedua kelas, termasuk kelas minoritas. Sebaliknya, model **Logistic Regression** memiliki kelemahan yang cukup signifikan dalam mendeteksi kelas minoritas, yang terlihat dari rendahnya nilai recall (0.07) dan F1-score (0.12) pada kelas tersebut.

Perbedaan utama antara kedua model ini adalah:
- **Akurasi Keseluruhan**: Random Forest (99.85%) jauh lebih tinggi dibandingkan Logistic Regression (80%).
- **Kinerja pada Kelas Minoritas**: Random Forest dapat memprediksi kelas minoritas dengan jauh lebih baik dibandingkan Logistic Regression, yang mengalami kesulitan dalam mendeteksi instance kelas ini.
- **General Performa**: Random Forest menghasilkan model yang lebih andal dan seimbang pada kedua kelas, sementara Logistic Regression tampak bias terhadap kelas mayoritas.

Secara keseluruhan, **Random Forest** adalah model yang lebih unggul untuk dataset ini dibandingkan dengan **Logistic Regression**.

## Inference
"""

# Simpan model
joblib.dump(rf_model, '/content/rf_model.joblib')

print("Model berhasil disimpan dalam format joblib")

"""### Membuat data sintetis

#### Pembuatan Data Sintetis dengan CTGAN

Dalam analisis data, seringkali kita dihadapkan pada situasi di mana data aktual terbatas atau tidak seimbang. Salah satu solusi yang efektif untuk mengatasi masalah ini adalah dengan menggunakan data sintetis, yang dapat membantu memperkaya dataset agar model pembelajaran mesin memiliki lebih banyak variasi dan memperoleh hasil yang lebih akurat.

Pada project ini, kami menggunakan **CTGAN (Conditional Tabular Generative Adversarial Network)** untuk menghasilkan data sintetis yang menyerupai distribusi data asli. CTGAN adalah metode berbasis Generative Adversarial Network (GAN) yang dirancang khusus untuk data tabular. Dengan CTGAN, kami dapat menghasilkan data sintetis yang mencerminkan pola, hubungan, dan distribusi data asli, termasuk data kategorikal maupun numerik.

Berikut adalah langkah-langkah dasar yang diambil untuk membuat data sintetis menggunakan CTGAN:
1. **Training Model CTGAN**: Model CTGAN dilatih pada dataset asli untuk mempelajari pola dan distribusi data tersebut.
2. **Membuat Data Sintetis**: Setelah proses pelatihan, model digunakan untuk menghasilkan data baru yang menyerupai data asli.
3. **Menggunakan Data Sintetis**: Data yang dihasilkan dapat digunakan untuk berbagai keperluan. Pada project ini, data sintetis digunakan untuk melakukan uji model (inference) yang telah dibuat.
"""

# Load model yang sudah disimpan
rf_model = joblib.load('/content/rf_model.joblib')

# Inisialisasi model CTGAN ke variabel baru
ctgan_model = CTGAN()

# Fit themodel CTGAN ke data asli
ctgan_model.fit(df)

# Membuat model CTGAN dari trained model
synthetic_data = ctgan_model.sample(10000)

# Pisahkan fitur dan label
X_synthetic = synthetic_data.drop('Exited', axis=1)
y_synthetic = synthetic_data['Exited']

"""Pada kode di atas, beberapa langkah diambil untuk memuat model, membuat data sintetis, dan mempersiapkannya untuk analisis lebih lanjut.

1. **Import Library yang Diperlukan**:
   - Library `joblib` digunakan untuk memuat model yang telah disimpan.
   - `pandas` digunakan untuk mengelola data dalam bentuk DataFrame.
   - `accuracy_score`, `confusion_matrix`, dan `classification_report` dari `sklearn.metrics` disiapkan untuk evaluasi model.
   - `seaborn` dan `matplotlib.pyplot` untuk visualisasi.
   - `CTGAN` dari library `ctgan` digunakan untuk menghasilkan data sintetis.

2. **Memuat Model yang Disimpan**:
   - Model `RandomForestClassifier` yang sebelumnya telah dilatih dan disimpan dengan `joblib` dimuat menggunakan `joblib.load('/content/rf_model.joblib')`.

3. **Inisialisasi Model CTGAN**:
   - `CTGAN` digunakan untuk membangkitkan data sintetis. Model ini diinisialisasi sebagai `ctgan_model`.

4. **Melatih CTGAN dengan Data Asli**:
   - `ctgan_model.fit(df)` digunakan untuk melatih model CTGAN dengan data asli (`df`). Proses ini penting untuk menghasilkan data sintetis yang memiliki pola serupa dengan data asli.

5. **Membuat Data Sintetis**:
   - `ctgan_model.sample(10000)` menghasilkan 10.000 baris data sintetis berdasarkan pola dari data asli yang telah dipelajari.

6. **Memisahkan Fitur dan Label**:
   - Setelah data sintetis dibuat, data tersebut dipisahkan menjadi variabel fitur (`X_synthetic`) dan label (`y_synthetic`). Label 'Exited' dipisahkan untuk mempersiapkan data bagi proses analisis atau pelatihan model lebih lanjut.

### Inference dengan Random forest

Pada kode berikut, dilakukan proses inferensi pada data sintetis menggunakan model `RandomForestClassifier` yang telah dilatih sebelumnya.
"""

# Lakukan inference pada data sintetis using the RandomForestClassifier model
predictions = rf_model.predict(X_synthetic)

# Menampilkan akurasi
accuracy = accuracy_score(y_synthetic, predictions)
print(f"Akurasi pada data sintetis: {accuracy:.2f}")

# Menampilkan confusion matrix
cm = confusion_matrix(y_synthetic, predictions)
print("Confusion Matrix:")
print(cm)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix on Synthetic Data')
plt.show()

# Menampilkan classification report
report = classification_report(y_synthetic, predictions)
print("Classification Report:")
print(report)

"""Berdasarkan hasil evaluasi, model `RandomForestClassifier` mencapai akurasi sebesar 0.95 atau 95% pada data sintetis. Berikut adalah penjelasan lebih detail dari hasil classification report:

- **Class 0** (label mayoritas):
  - **Precision**: 0.96, menunjukkan bahwa 96% dari prediksi untuk kelas ini benar.
  - **Recall**: 0.96, menunjukkan bahwa model berhasil mengidentifikasi 96% dari semua instance sebenarnya dalam kelas ini.
  - **F1-Score**: 0.96, yang merupakan kombinasi dari precision dan recall, menunjukkan performa model yang konsisten pada kelas ini.

- **Class 1** (label minoritas):
  - **Precision**: 0.83, menunjukkan bahwa 83% dari prediksi untuk kelas ini benar.
  - **Recall**: 0.90, menunjukkan bahwa model berhasil mengidentifikasi 90% dari semua instance sebenarnya dalam kelas ini.
  - **F1-Score**: 0.84, yang menunjukkan kinerja yang baik meskipun lebih rendah daripada kelas mayoritas.

- **Akurasi Keseluruhan**: Model mencapai akurasi 94%, yang berarti bahwa 94% prediksi pada data sintetis adalah benar.
  
- **Macro Average**: Nilai rata-rata precision, recall, dan F1-score untuk kedua kelas adalah 0.90, yang memberikan gambaran kinerja rata-rata untuk masing-masing kelas.
  
- **Weighted Average**: Nilai rata-rata precision, recall, dan F1-score dengan mempertimbangkan proporsi setiap kelas adalah 0.94, yang menunjukkan bahwa model memiliki kinerja keseluruhan yang baik, terutama pada kelas mayoritas.

Secara keseluruhan, model menunjukkan performa yang sangat baik dalam memprediksi kelas mayoritas, dengan performa yang cukup baik juga untuk kelas minoritas.

### Inference dengan logistic regression
"""

# Melakukan prediksi pada data sintetis
synthetic_data_for_prediction = synthetic_data[X_synthetic.columns]

# Prediksi menggunakan data sintetis
synthetic_predictions = log_reg.predict(synthetic_data_for_prediction)

# Asumsikan synthetic_y sebagai target untuk data sintetis
accuracy = accuracy_score(y_synthetic, synthetic_predictions)
print(f"Akurasi pada data sintetis: {accuracy:.2f}")

# Menampilkan confusion matrix
cm = confusion_matrix(y_synthetic, synthetic_predictions)
print("Confusion Matrix pada Data Sintetis:")
print(cm)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix on Synthetic Data')
plt.show()

# Menampilkan classification report
report = classification_report(y_synthetic, synthetic_predictions)
print("Classification Report pada Data Sintetis:")
print(report)

"""Model **Logistic Regression** menghasilkan akurasi sebesar 0.80 atau 80% pada data sintetis. Berikut adalah ringkasan hasil evaluasi berdasarkan classification report:

- **Class 0** (label mayoritas):
  - **Precision**: 0.80, yang berarti 80% prediksi kelas 0 adalah benar.
  - **Recall**: 0.99, menunjukkan bahwa model berhasil mengidentifikasi 99% dari semua instance sebenarnya di kelas ini.
  - **F1-Score**: 0.89, menunjukkan performa yang baik pada kelas mayoritas.

- **Class 1** (label minoritas):
  - **Precision**: 0.16, menunjukkan bahwa hanya 16% dari prediksi kelas 1 adalah benar.
  - **Recall**: 0.00, menunjukkan model hanya mengidentifikasi 0% dari instance yang sebenarnya di kelas ini.
  - **F1-Score**: 0.01, yang menunjukkan performa yang lemah pada kelas ini.

- **Akurasi Keseluruhan**: Model mencapai akurasi 80%, yang menunjukkan bahwa 80% prediksi pada data sintetis adalah benar.

- **Macro Average**: Rata-rata precision, recall, dan F1-score untuk kedua kelas adalah sekitar 0.48, 0.5, dan 0.45. Nilai ini menunjukkan bahwa kinerja pada kedua kelas, terutama kelas minoritas, masih rendah.

- **Weighted Average**: Nilai rata-rata precision, recall, dan F1-score dengan mempertimbangkan proporsi masing-masing kelas adalah sekitar 0.67, 0.80, dan 0.71, yang menunjukkan performa yang lebih baik pada kelas mayoritas.

## Kesimpulan  

Model **Random Forest** yang diuji pada data sintetis sebelumnya memberikan hasil yang jauh lebih baik, dengan akurasi sebesar 95% dan kinerja yang lebih seimbang pada kedua kelas, termasuk kelas minoritas.

Perbedaan utama antara kedua model ini adalah:
- **Akurasi Keseluruhan**: Random Forest (95%) lebih tinggi dibandingkan Logistic Regression (80%).
- **Kinerja pada Kelas Minoritas**: Random Forest menunjukkan performa yang lebih baik dalam memprediksi kelas minoritas dibandingkan Logistic Regression, yang mengalami kesulitan dengan nilai recall yang rendah (0.18) pada kelas tersebut.
- **General Performa**: Random Forest memberikan hasil yang lebih andal dan seimbang untuk kedua kelas, sementara Logistic Regression cenderung bias terhadap kelas mayoritas.

Secara keseluruhan, **Random Forest** menunjukkan kinerja yang lebih unggul pada data sintetis dibandingkan dengan **Logistic Regression**, terutama dalam menangani kelas minoritas.
"""